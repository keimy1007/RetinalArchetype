{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: run VAE+AA for Harvard-GDP datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "matplotlib.rcParams['font.weight'] = 'bold'\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "matplotlib.rcParams['figure.titlesize'] = 18\n",
    "# matplotlib.rcParams['font.style'] = 'italic'\n",
    "\n",
    "# utility\n",
    "import time\n",
    "import copy as cp\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from sys import stderr\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# 乱数\n",
    "rng = np.random.RandomState(42)\n",
    "random_state = 42\n",
    "\n",
    "# %cd your path\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_name = \"pRNFL_Harvard\"\n",
    "df_oct = pd.read_csv(f\"datasets/data_all_sample.csv\")\n",
    "\n",
    "# add columns\n",
    "df_oct[\"mrt\"] = df_oct.iloc[:, 7:683].astype(float).mean(axis=1)\n",
    "df_oct[\"ageclass\"] = df_oct[\"age\"].apply(lambda x: x // 5)\n",
    "\n",
    "print(df_oct.shape)\n",
    "df_oct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# no split\n",
    "df_train, df_test = df_oct, df_oct\n",
    "df_train.to_csv(\"datasets/data_train.csv\", index=False)\n",
    "df_test.to_csv(\"datasets/data_test.csv\", index=False)\n",
    "\n",
    "print(\"train\", df_train.shape)\n",
    "print(\"test\", df_test.shape)\n",
    "df_train.race.value_counts(normalize=True), df_test.race.value_counts(normalize=True)\n",
    "\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "\n",
    "        X = item.iloc[7:683].values.astype(float)\n",
    "        vmin,vmax = (0,240)\n",
    "        X = (X - vmin) / (vmax - vmin)\n",
    "        X = np.clip(X, 0, 1)\n",
    "        X = X.reshape(1, 26, 26)\n",
    "\n",
    "        ID = float(item['ID'])\n",
    "        ageclass = float(item['ageclass'])\n",
    "        sex = float(item['sex'])\n",
    "        mrt = float(item['mrt'])\n",
    "        \n",
    "        return X, ID, ageclass, sex, mrt\n",
    "\n",
    "def custom_collate(batch):\n",
    "    X, ID, ageclass, sex, mrt = zip(*batch)\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    ID = torch.tensor(ID, dtype=torch.float32)\n",
    "    ageclass = torch.tensor(ageclass, dtype=torch.float32)\n",
    "    sex = torch.tensor(sex, dtype=torch.float32)\n",
    "    mrt = torch.tensor(mrt, dtype=torch.float32)\n",
    "    return X, ID, ageclass, sex, mrt\n",
    "\n",
    "\n",
    "def dataframe2dataloader(df, batch_size, shuffle=False, random_state=42):\n",
    "    dataset = CustomDataset(df)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=custom_collate)\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset, train_loader = dataframe2dataloader(df_train, batch_size, shuffle=True, random_state=random_state)\n",
    "valid_dataset, valid_loader = dataframe2dataloader(df_test, batch_size, shuffle=False, random_state=random_state)\n",
    "\n",
    "for X, ID, ageclass, sex, mrt in train_loader:\n",
    "    print(f\"X Shape: {X.shape}\")\n",
    "    print(f\"ID: {ID.shape}\")\n",
    "    print(f\"Ageclass: {ageclass.shape}\")\n",
    "    print(f\"Sex: {sex.shape}\")\n",
    "    print(f\"mRT: {mrt.shape}\")\n",
    "    break          \n",
    "\n",
    "for X, ID, ageclass, sex, mrt in valid_loader:\n",
    "    print(f\"X Shape: {X.shape}\")\n",
    "    print(f\"ID: {ID.shape}\")\n",
    "    print(f\"Ageclass: {ageclass.shape}\")\n",
    "    print(f\"Sex: {sex.shape}\")\n",
    "    print(f\"mRT: {mrt.shape}\")\n",
    "    break\n",
    "\n",
    "print()\n",
    "print(\"train\", len(train_dataset))\n",
    "print(\"valid\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Tuple\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim=128, indim=(26, 26), c_init=1, outdim_conv=8):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(c_init, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ln1 = nn.LayerNorm([2, indim[0], indim[1]])\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ln2 = nn.LayerNorm([4, indim[0], indim[1]])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ln3 = nn.LayerNorm([4, indim[0], indim[1]])\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(4, outdim_conv, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.ln4 = nn.LayerNorm([outdim_conv, indim[0], indim[1]])\n",
    "\n",
    "        self.fc1 = nn.Linear(outdim_conv * indim[0] * indim[1], h_dim)\n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        \n",
    "        self.fc_mean = nn.Linear(h_dim, z_dim)\n",
    "        self.fc_var = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ln1(self.conv1(x)))\n",
    "        x = F.relu(self.ln2(self.conv2(x))) \n",
    "        x = F.relu(self.ln3(self.conv3(x)))  \n",
    "        x = F.relu(self.ln4(self.conv4(x))) \n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x) \n",
    "        \n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        \n",
    "        return mean, log_var\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim=128, indim=(26, 26), c_init=1, indim_deconv=8):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, indim_deconv * indim[0] * indim[1])\n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(indim_deconv, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([4, indim[0], indim[1]])\n",
    "        \n",
    "        self.deconv2 = nn.ConvTranspose2d(4, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([4, indim[0], indim[1]])\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(4, 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln3 = nn.LayerNorm([2, indim[0], indim[1]])\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(2, c_init, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z, indim=(26, 26), indim_deconv=8):\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.dropout(z)  \n",
    "        \n",
    "        z = z.view(z.size(0), indim_deconv, indim[0], indim[1])\n",
    "        \n",
    "        z = F.relu(self.ln1(self.deconv1(z)))\n",
    "        z = F.relu(self.ln2(self.deconv2(z)))\n",
    "        z = F.relu(self.ln3(self.deconv3(z)))\n",
    "        reconstruction = torch.sigmoid(self.deconv4(z))\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "class VAE_CNN(nn.Module):\n",
    "    def __init__(self, z_dim=64, h_dim=128):\n",
    "        super(VAE_CNN, self).__init__()\n",
    "        self.encoder = CNNEncoder(z_dim, h_dim)\n",
    "        self.decoder = CNNDecoder(z_dim, h_dim)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        return self.decoder(z), mean, log_var\n",
    "\n",
    "    def loss(self, x):\n",
    "        recon_x, mean, log_var = self.forward(x)\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction='mean', size_average=False)\n",
    "        KLD = -0.5 * torch.mean(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "        return KLD, BCE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "save_path = \"params/VAE/pretrained_for_pRNFL.pth\"\n",
    "\n",
    "model_vae = VAE_CNN(z_dim = z_dim).to(device)\n",
    "model_vae.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# configurement\n",
    "n_epochs = 100\n",
    "beta = 100\n",
    "tolerance = 3  # Early stopping tolerance\n",
    "optimizer = optim.Adam(model_vae.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "train_KL_losses = []\n",
    "train_reconstruction_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None  \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    KL_losses = []\n",
    "    reconstruction_losses = []\n",
    "\n",
    "    model_vae.train()\n",
    "    with tqdm(total=len(train_loader), leave=False) as pbar:\n",
    "        for i, (x, id, ageclass, sex, mrt) in enumerate(train_loader):\n",
    "            x = x.float().to(device)  \n",
    "            model_vae.zero_grad()\n",
    "\n",
    "            KL_loss, reconstruction_loss = model_vae.loss(x)\n",
    "            loss = beta * KL_loss + reconstruction_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save loss\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            KL_losses.append(KL_loss.cpu().detach().numpy())\n",
    "            reconstruction_losses.append(reconstruction_loss.cpu().detach().numpy())\n",
    "            \n",
    "            # progress bar\n",
    "            pbar.set_postfix({'loss': loss.item(), 'KL_loss': KL_loss.item(), 'reconstruction_loss': reconstruction_loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "    train_losses.append(losses)\n",
    "    train_KL_losses.append(KL_losses)\n",
    "    train_reconstruction_losses.append(reconstruction_losses)\n",
    "\n",
    "    losses_val = []\n",
    "    model_vae.eval()\n",
    "    with tqdm(total=len(valid_loader), leave=False) as pbar:\n",
    "        for i, (x, id, ageclass, sex, mrt) in enumerate(valid_loader):\n",
    "            x = x.float().to(device)  \n",
    "            KL_loss, reconstruction_loss = model_vae.loss(x)\n",
    "            loss = KL_loss + reconstruction_loss\n",
    "\n",
    "            # save loss\n",
    "            losses_val.append(loss.cpu().detach().numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "    val_losses.append(losses_val)\n",
    "\n",
    "    # loss\n",
    "    avg_train_loss = sum(losses) / len(losses)\n",
    "    avg_val_loss = sum(losses_val) / len(losses_val)\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model_vae.state_dict().copy()\n",
    "        print(f\"Best validation loss updated to {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= tolerance:\n",
    "        print(f\"Early stopping triggered after {tolerance} epochs without improvement\")\n",
    "        break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model_vae.load_state_dict(best_model_state)\n",
    "    print(f\"Restored model_vae to best state with validation loss: {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"params/VAE\", exist_ok=True)\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime('%Y%m%d%H%M')\n",
    "avg_train_losses = [sum(epoch_losses)/len(epoch_losses) for epoch_losses in train_losses]\n",
    "avg_train_KL_losses = [sum(epoch_losses)/len(epoch_losses) for epoch_losses in train_KL_losses]\n",
    "avg_train_reconstruction_losses = [sum(epoch_losses)/len(epoch_losses) for epoch_losses in train_reconstruction_losses]\n",
    "avg_val_losses = [sum(epoch_losses)/len(epoch_losses) for epoch_losses in val_losses]\n",
    "\n",
    "best_epoch = len(avg_val_losses) - epochs_no_improve - 1 if epochs_no_improve > 0 else len(avg_val_losses) - 1\n",
    "best_epoch = max(0, best_epoch)  \n",
    "\n",
    "save_path = f\"params/VAE/finetunig_for_pRNFL.pth\"\n",
    "torch.save(model_vae.state_dict(), save_path)\n",
    "\n",
    "# figure\n",
    "plt.figure(figsize=(12, 6))\n",
    "epochs = list(range(1, len(avg_train_losses) + 1))\n",
    "plt.plot(epochs, avg_train_losses, label='Train Loss')\n",
    "plt.plot(epochs, avg_val_losses, label='Validation Loss')\n",
    "\n",
    "plt.axvline(x=best_epoch+1, color='r', linestyle='--', \n",
    "            label=f'Best Model (Epoch {best_epoch+1})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xticks(range(1, len(avg_train_losses) + 1, 1))\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/LOSS_model_{current_time}.pdf', transparent=True)\n",
    "\n",
    "# csv\n",
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "is_best = [False] * len(train_losses)\n",
    "is_best[best_epoch] = True \n",
    "\n",
    "df_loss = pd.DataFrame({\n",
    "    \"epoch\": epochs,\n",
    "    \"train_loss\": avg_train_losses,\n",
    "    \"train_KL_loss\": avg_train_KL_losses,\n",
    "    \"train_reconstruction_loss\": avg_train_reconstruction_losses,\n",
    "    \"val_loss\": avg_val_losses,\n",
    "    \"is_best_model\": is_best\n",
    "})\n",
    "\n",
    "df_loss.to_csv(f\"tables/LOSS_model_{current_time}.csv\", index=False)\n",
    "\n",
    "with open(f\"tables/MODEL_INFO_{current_time}.txt\", 'w') as f:\n",
    "    f.write(f\"Model: VAE\\n\")\n",
    "    f.write(f\"OCT Name: {oct_name}\\n\")\n",
    "    f.write(f\"z_dim: {z_dim}\\n\")\n",
    "    f.write(f\"beta: {beta}\\n\")\n",
    "    f.write(f\"Max Epochs: {n_epochs}\\n\")\n",
    "    f.write(f\"Actual Epochs: {len(train_losses)}\\n\")\n",
    "    f.write(f\"Best Epoch: {best_epoch+1}\\n\")\n",
    "    f.write(f\"Best Validation Loss: {avg_val_losses[best_epoch]:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference: reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def visualize(arr, vmin=0, vmax=1, figname=None):\n",
    "    matrix = np.reshape(arr, (26, 26))\n",
    "\n",
    "    base_cmap = plt.cm.RdYlBu_r\n",
    "    colors = base_cmap(np.arange(base_cmap.N))\n",
    "    new_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(matrix, cmap=new_cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.title(\"\")\n",
    "\n",
    "    plt.savefig(f\"figures/samples_VAE/{figname}.pdf\", transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "def show_ranges(arr): return arr.min(), arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"figures/samples_VAE\", exist_ok=True)\n",
    "\n",
    "model_vae.eval()\n",
    "\n",
    "n = len(valid_dataset)\n",
    "# n = 100\n",
    "for i in tqdm(range(n)):\n",
    "    i *= 1\n",
    "\n",
    "    x, ID, ageclass, sex, mrt = valid_dataset[i]\n",
    "\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = x.unsqueeze(0)\n",
    "    x = x.to(device)\n",
    "\n",
    "    ID = int(ID)\n",
    "\n",
    "    y, z_mu, z_logvar = model_vae(x)\n",
    "    KLD, BCE = model_vae.loss(x)\n",
    "\n",
    "    x = x.cpu().detach().numpy()\n",
    "    y = y.cpu().detach().numpy()\n",
    "    BCE = BCE.cpu().detach().numpy().astype(int)\n",
    "\n",
    "    visualize(x.reshape(676), vmin=0, vmax=1, figname=f\"{oct_name}_{ID}_origin_{BCE}\")\n",
    "    visualize(y.reshape(676), vmin=0, vmax=1, figname=f\"{oct_name}_{ID}_recon_{BCE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference: datasets with Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def extract_features_from_dataset(dataset, model, device, z_dim=32, prefix='data'):\n",
    "    data = []\n",
    "    \n",
    "    for i, (x, id, ageclass, sex, mrt) in enumerate(tqdm(dataset, desc=f\"Processing {prefix} data\")):\n",
    "        x = torch.from_numpy(x).float() if isinstance(x, np.ndarray) else x.float()\n",
    "        x = x.unsqueeze(0) if len(x.shape) == 3 else x  # バッチ次元がない場合は追加\n",
    "        x = x.to(device)\n",
    "        \n",
    "        with torch.no_grad():  \n",
    "            y, z_mu, z_logvar = model_vae(x)\n",
    "            kl_loss, bce_loss = model_vae.loss(x)\n",
    "        \n",
    "        id = int(id)\n",
    "        ageclass = int(ageclass)\n",
    "        sex = int(sex)\n",
    "        mrt = float(mrt)\n",
    "        z = z_mu.cpu().detach().numpy()[0]\n",
    "        bce_loss = float(bce_loss.cpu().detach().numpy())\n",
    "        \n",
    "        data.append([id, ageclass, sex, mrt, bce_loss] + list(z))\n",
    "    \n",
    "    columns = ['ID', 'ageclass', 'sex', 'mrt', 'bce_loss'] + [f'z{i}' for i in range(1, z_dim + 1)]\n",
    "    \n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "model_vae.eval()  \n",
    "z_dim = 32\n",
    "\n",
    "# training data feature\n",
    "df_train = extract_features_from_dataset(\n",
    "    dataset=train_dataset, \n",
    "    model=model_vae, \n",
    "    device=device,\n",
    "    z_dim=z_dim,\n",
    "    prefix=\"train\"\n",
    ")\n",
    "\n",
    "# save\n",
    "train_path = f'datasets/data_train_zdim.csv'\n",
    "df_train.to_csv(train_path, index=False)\n",
    "print(f\"Saved training features to {train_path}\")\n",
    "\n",
    "\n",
    "# validation data feature\n",
    "df_valid = extract_features_from_dataset(\n",
    "    dataset=valid_dataset,\n",
    "    model=model_vae,\n",
    "    device=device,\n",
    "    z_dim=z_dim,\n",
    "    prefix=\"validation\"\n",
    ")\n",
    "\n",
    "# save\n",
    "valid_path = f'datasets/data_test_zdim.csv'\n",
    "df_valid.to_csv(valid_path, index=False)\n",
    "print(f\"Saved validation features to {valid_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA model (with data filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archetypes import AA\n",
    "from tqdm import tqdm\n",
    "\n",
    "def perform_archetypal_analysis(X_train, X_test, n_archetypes=10, n_init=1, max_iter=1000, random_state=42):\n",
    "    model_aa = AA(n_archetypes=n_archetypes, n_init=n_init, max_iter=max_iter, random_state=random_state)   \n",
    "    model_aa.fit(X_train)\n",
    "    \n",
    "    A = model_aa.archetypes_\n",
    "    RSS = model_aa.rss_\n",
    "    Z2 = model_aa.transform(X_test)\n",
    "    \n",
    "    print(\"A shape:\", A.shape, \"RSS:\", RSS)\n",
    "    \n",
    "    return A, Z2, RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering OCTs by BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_name = \"pRNFL_Harvard\"\n",
    "\n",
    "df_train = pd.read_csv(f'datasets/data_train_zdim.csv')\n",
    "df_test = pd.read_csv(f'datasets/data_test_zdim.csv')\n",
    "\n",
    "def exclude_abnormal(df):\n",
    "    mean_bce = df['bce_loss'].mean()\n",
    "    std_bce = df['bce_loss'].std()\n",
    "    df['z_score'] = (df['bce_loss'] - mean_bce) / std_bce\n",
    "    \n",
    "    df_filtered = df[(df['z_score'] > -1.96) & (df['z_score'] < 1.96)].copy()    \n",
    "    df_filtered.drop(columns=['z_score'], inplace=True)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "df_train_filtered = exclude_abnormal(df_train)\n",
    "df_valid_filtered = exclude_abnormal(df_valid)\n",
    "\n",
    "df_train_filtered.to_csv(f'datasets/data_train_zdim_filtered.csv', index=False)\n",
    "df_valid_filtered.to_csv(f'datasets/data_test_zdim_filtered.csv', index=False)\n",
    "\n",
    "df_train.shape, df_test.shape, df_train_filtered.shape, df_valid_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"tables/AT\", exist_ok=True)\n",
    "os.makedirs(\"datasets/AT_data\", exist_ok=True)\n",
    "os.makedirs(\"datasets/AT_info\", exist_ok=True)\n",
    "\n",
    "df_train = df_train_filtered\n",
    "df_test = df_valid_filtered\n",
    "\n",
    "X_train = df_train.iloc[:, 5:].values\n",
    "X_test = df_test.iloc[:, 5:].values\n",
    "\n",
    "# AA model\n",
    "RSS_lists = []\n",
    "for k in range(3, 15):\n",
    "    A, Z, RSS = perform_archetypal_analysis(X_train, X_test, n_archetypes=k, max_iter=1000)\n",
    "    A_df = pd.DataFrame(A.T, columns=[f'A{i+1}' for i in range(k)])\n",
    "    Z_df = pd.DataFrame(Z, columns=[f'A{i+1}' for i in range(k)])\n",
    "    RSS_lists.append(RSS) \n",
    "\n",
    "    # save as csv\n",
    "    A_df.to_csv(f\"datasets/AT_info/{oct_name}_AT{k}_info.csv\", index=False)\n",
    "    pd.concat([df_test.reset_index(drop=True), Z_df.reset_index(drop=True)], axis=1).to_csv(f\"datasets/AT_data/data_test_AT{k}.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(RSS_lists, columns=['RSS']).to_csv(f\"datasets/AT_info/{oct_name}_RSS.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSS plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"figures/AT\", exist_ok=True)\n",
    "\n",
    "RSS_lists = pd.read_csv(f\"tables_revise/AT/{oct_name}_RSS.csv\").values.flatten()\n",
    "\n",
    "max_k = 15\n",
    "\n",
    "# RSS plots\n",
    "figure = plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(3, max_k), RSS_lists[:], marker='o', color=\"blue\", lw=2, markersize=10)\n",
    "plt.xlabel('Number of Archetypes')\n",
    "plt.ylabel('RSS')\n",
    "plt.xticks(range(3, max_k))\n",
    "\n",
    "title_name = \"mGCIPL\" if oct_name == \"mGCLP\" else oct_name\n",
    "plt.title(f'{title_name}', fontsize=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.17, right=0.97, bottom=0.15, top=0.9)\n",
    "plt.savefig(f\"figures/AT/RSS_{oct_name}.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(arr, vmin=0, vmax=1, figname=None, title=None):\n",
    "    matrix = np.reshape(arr, (26, 26))\n",
    "    base_cmap = plt.cm.RdYlBu_r\n",
    "\n",
    "    colors = base_cmap(np.arange(base_cmap.N))\n",
    "    new_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(matrix, cmap=new_cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.title(f\"{title}\", fontsize=40)\n",
    "\n",
    "    plt.savefig(f\"figures/AT/AT_maps_K={k}/{figname}.pdf\", transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "def show_ranges(arr): return arr.min(), arr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k =  12\n",
    "oct_name = \"pRNFL_Harvard\"\n",
    "\n",
    "import os\n",
    "os.makedirs(f'figures/AT/AT_maps_K={k}', exist_ok=True)\n",
    "\n",
    "A_df = pd.read_csv(f\"datasets/AT_info/pRNFL_Harvard_AT{k}_info.csv\")\n",
    "\n",
    "for idx in range(k):\n",
    "    z = A_df.values[:, idx]\n",
    "    z = torch.from_numpy(z).float()\n",
    "    z = z.unsqueeze(0)\n",
    "    z = z.to(device)\n",
    "\n",
    "    y = model_vae.decoder(z)\n",
    "    y = y.cpu().detach().numpy()\n",
    "\n",
    "    visualize(y.reshape(676), vmin=0, vmax=1, figname=f\"{oct_name}_A{idx+1}\", title=f\"A{idx+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
